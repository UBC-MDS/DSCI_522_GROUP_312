{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Median House Prices in California Census Blocks (Census 1990)\n",
    "\n",
    "Author: DSCI 522 Group 312\n",
    "\n",
    "Date: January 25, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis focuses on predicting the median house prices in census blocks given independent variable about the location, home characteristics, and the demographics of the census block. This dataset was sourced from Kaggle, and many other people have completed [similar analyses](https://www.kaggle.com/camnugent/california-housing-prices/kernels).\n",
    "\n",
    "Our goal is to build a model that will predict median house value with a higher model validation score than the 0.60 achieved by [Eric Chen](https://www.kaggle.com/ericfeng84), the author of [The California House Price](https://www.kaggle.com/ericfeng84/the-california-housing-price) Kaggle page from which the dataset was obtained.\n",
    "\n",
    "We aim to bring additional insight to the existing models including looking at multicollinearity and trying KNN with a variety of different values for n_neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Data\n",
    "This dataset is a modified version of [The California Housing Dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), with [additional columns added by Aurélien Geron](https://github.com/ageron/handson-ml). This dataset contains information about median California house values per census block as sourced from the 1990 US Census.\n",
    "\n",
    "### Analysis\n",
    "We used Linear Regression, K-Nearest Neighbour Trees, and a Random Forest Regressor to predict the median house value given the independent variables.\n",
    "\n",
    "### Results and Discussion\n",
    "The Exploratory Data Analysis focused on identifying linear relationships between the independent variables and the dependent variable as well as looking at correlations between independent variables. Previous analyses of this dataset highlighted that linear regression was an appropriate prediction method for the median housing value, but generally, they lacked insight into multicollinearity (the correlation and linear relationships between independent variables). Of all of the variables examined, the Variance Inflation Factor (VIF) was higher than 1, which means that there is strong evidence of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housing_median_age</td>\n",
       "      <td>1.163905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>total_rooms</td>\n",
       "      <td>11.849443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_bedrooms</td>\n",
       "      <td>34.891047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>population</td>\n",
       "      <td>6.582837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>households</td>\n",
       "      <td>33.871693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>median_income</td>\n",
       "      <td>1.524263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intercept</td>\n",
       "      <td>18.278944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             variable        VIF\n",
       "0  housing_median_age   1.163905\n",
       "1         total_rooms  11.849443\n",
       "2      total_bedrooms  34.891047\n",
       "3          population   6.582837\n",
       "4          households  33.871693\n",
       "5       median_income   1.524263\n",
       "6           intercept  18.278944"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('eda_charts/vif_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable with the highest VIF is total bedrooms, and this appears to be strongly linearly related to the total number of rooms, given that the room count includes the bedrooms.\n",
    "\n",
    "![Image](eda_charts/total-rooms_total-bedrooms.png)\n",
    "\n",
    "The following heatmap represents the correlation values of the variables.\n",
    "\n",
    "![Image](eda_charts/correlation_heatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to address multicollinearity is to remove variables with high VIFs. As is common in this case, the Linear Regression model performed best when all (or all but one) of the variables were included. The following illustrates the Recursive Feature Elimination for a Linear Regression Model. The x-axis represents the number of features selected so far.\n",
    "\n",
    "We additionally ran Recursive Feature Elimination on a Linear Regression model, excluding Latitude and Longitude, since these features are very specific to California. The results follow.\n",
    "\n",
    "<img src=\"ml_results/LR_performace.png\" width=\"50%\" align=\"left\"/> <img src=\"ml_results/LR_performace_exc_feats.png\" width=\"50%\" align=\"left\"/>\n",
    "\n",
    "It is clear that Linear Regression performed more favourably on the training and testing data including latitude and longitude. This is somewhat to be expected, as areas with expensive median house values often border other areas with similar socioeconomic groups.\n",
    "\n",
    "To attempt to address the multicollinearity, we also ran Recursive Feature Elimination excluding longitude, latitude, and total bedrooms, which was the feature that had the highest Variance Inflation Factor.\n",
    "\n",
    "![Image](ml_results/LR_performace_exc_feats_2.png)\n",
    "\n",
    "As expected, the results are very similar to the model that only excluded latitude and longitude because the information from the feature \"total_bedrooms\" is effectively redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to fit a K-Nearest Neighbor to our data, and KNN yielded better accuracy than simple linear regression. A Standard Scaler was used to pre-process the data, which likely contributed to the success of KNN. The following demonstrates the relationship between the number of nearest neighbours and the resulting training and testing scores.\n",
    "\n",
    "![Image](ml_results/KNN_performace.png)\n",
    "\n",
    "\n",
    "As with Linear Regression, we removed Latitude and Longitude in hopes to see the effect it had on KNN in terms of spatial nearest neightbours, and the results are as follows.\n",
    "\n",
    "![Image](ml_results/KNN_performace_exc_feats.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can infer that having latitude and longitude included did improve the KNN model. With or without these features, the number of nearest neighbours that should be used is approximately 9 in order to avoid overfitting.\n",
    "\n",
    "The goal of our project is not to predict based on Census data for other states, however the results are still quite effective without latitude and longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas for Improvement\n",
    "Opportunities for improvement of the predictive model include:\n",
    "- Increasing the breadth of Machine Learning Models used to predict the median housing value;\n",
    "- Obtaining cross-validation scores, rather than the simple score;\n",
    "- Work through a feature engineering process to make features more relevant;\n",
    "- Use a feature selection method for linear regression that addresses or discourages multicollinearity;\n",
    "- Conducting more in-depth analysis to address multicollinearity.\n",
    "\n",
    "### Conclusion\n",
    "In both our linear regression model and K-Nearest Neighbors model, we achieved higher accuracy than Eric Chen's best score, which was with linear regression. Chen did not fit a KNN model, so it is unclear whether this model would have performed better for him. For the purposes of predicting median housing price in California by census block, the linear regression and KNN models are effective at estimating the response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Balla, Deepanshu. n.d. SPLITTING Data into Training and Test Sets with R. https://www.listendata.com/2015/02/splitting-data-into-training-and-test.html.\n",
    "\n",
    "de Jonge, Edwin 2018. docopt: Command-Line Interface Specification Language. https://CRAN.R-project.org/package=docopt.\n",
    "\n",
    "Kuhn, Max. 2020. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret.\n",
    "\n",
    "Lang, Michael 2017. checkmate: Fast Argument Checks for Defensive R Programming. https://journal.r-project.org/archive/2017/RJ-2017-028/index.html.\n",
    "\n",
    "R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n",
    "\n",
    "Wickham, Hadley. 2011. testthat: Get Started with Testing. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n",
    "\n",
    "Wickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse.\n",
    "\n",
    "Wickham, Hadley, and Lionel Henry. 2019. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n",
    "\n",
    "Wickham, Hadley, Jim Hester, and Romain Francois. 2018. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n",
    "\n",
    "Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay; 12(Oct):2825−2830, 2011 https://scikit-learn.org/stable/\n",
    "\n",
    "Bernard J. (2016) Python Data Analysis with pandas. In: Python Recipes Handbook. Apress, Berkeley, CA. https://pandas.pydata.org/\n",
    "\n",
    "Pedregosa et al., 2011. Scikit-learn: Machine Learning in Python, JMLR 12, pp. 2825-2830. http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n",
    "\n",
    "Seabold, Skipper, and Josef Perktold, 2010. “statsmodels: Econometric and statistical modeling with python.” Proceedings of the 9th Python in Science Conference. http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf\n",
    "\n",
    "VanderPlas, Jacob & Granger, Brian & Heer, Jeffrey & Moritz, Dominik & Wongsuphasawat, Kanit & Lees, Eitan & Timofeev, Ilia & Welsh, Ben & Sievert, Scott. (2018). Altair: Interactive Statistical Visualizations for Python. Journal of Open Source Software. 3. 1057. 10.21105/joss.01057. https://altair-viz.github.io/_sources/index.rst.txt\n",
    "\n",
    "plightbo, simon.m.stewart, hbchai, jrhuggins, et al. © Copyright 2011. https://selenium.dev/documentation/en/front_matter/copyright_and_attributions/\n",
    "\n",
    "Oliphant, T. E. (2006). A guide to NumPy (Vol. 1). Trelgol Publishing USA. https://web.mit.edu/dvp/Public/numpybook.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
