{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Median House Prices in California Census Blocks (Census 1990)\n",
    "\n",
    "Author: DSCI 522 Group 312\n",
    "\n",
    "Date: January 25, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This analysis focuses on predicting the median house prices in census blocks given independent variable about the location, home characteristics, and the demographics of the census block. This dataset was sourced from Kaggle, and many other people have completed [similar analyses](https://www.kaggle.com/camnugent/california-housing-prices/kernels).\n",
    "\n",
    "Our goal is to build a model that will predict median house value with a higher model validation score than the 0.60 achieved by [Eric Chen](https://www.kaggle.com/ericfeng84), the author of [The California House Price](https://www.kaggle.com/ericfeng84/the-california-housing-price) Kaggle page from which the dataset was obtained.\n",
    "\n",
    "We aim to bring additional insight to the existing models including looking at multicollinearity and trying KNN with a variety of different values for n_neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods\n",
    "\n",
    "### Data\n",
    "This dataset is a modified version of [The California Housing Dataset](https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html), with [additional columns added by Aur√©lien Geron](https://github.com/ageron/handson-ml). This dataset contains information about median California house values per census block as sourced from the 1990 US Census.\n",
    "\n",
    "### Analysis\n",
    "We used Linear Regression, K-Nearest Neighbour, and a Random Forest Regressor to predict the median household value given the independent variables.\n",
    "\n",
    "### Results and Discussion\n",
    "The Exploratory Data Analysis focused on identifying linear relationships between the independent variables and the dependent variable as well as looking at correlations between independent variables. Previous analyses of this dataset highlighted that linear regression was an appropriate prediction method for the median housing value, but generally, they lacked insight into multicollinearity (the correlation and linear relationships between independent variables). Of all of the variables examined, the Variance Inflation Factor (VIF) was higher than 1, which means that there is strong evidence of multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>VIF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housing_median_age</td>\n",
       "      <td>1.163905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>total_rooms</td>\n",
       "      <td>11.849443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>total_bedrooms</td>\n",
       "      <td>34.891047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>population</td>\n",
       "      <td>6.582837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>households</td>\n",
       "      <td>33.871693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>median_income</td>\n",
       "      <td>1.524263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intercept</td>\n",
       "      <td>18.278944</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             variable        VIF\n",
       "0  housing_median_age   1.163905\n",
       "1         total_rooms  11.849443\n",
       "2      total_bedrooms  34.891047\n",
       "3          population   6.582837\n",
       "4          households  33.871693\n",
       "5       median_income   1.524263\n",
       "6           intercept  18.278944"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('eda_charts/vif_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable with the highest VIF is total bedrooms, and this appears to be strongly linearly related to the total number of rooms, given that the room count includes the bedrooms.\n",
    "\n",
    "![Image](eda_charts/total-rooms_total-bedrooms.png)\n",
    "\n",
    "The following heatmap represents the correlation values of the variables.\n",
    "\n",
    "<img src=\"eda_charts/correlation_heatmap.png\" width=\"80%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common approach to address multicollinearity is to remove variables with high VIFs. As is common in this case, the Linear Regression model performed best when all (or all but one) of the variables were included. The following illustrates the Recursive Feature Elimination for a Linear Regression Model. The x-axis represents the number of features selected so far.\n",
    "\n",
    "We additionally ran Recursive Feature Elimination on a Linear Regression model, excluding Latitude and Longitude, since these features are very specific to California. The results follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_results/LR_performace.png\" width=\"40%\" align=\"left\"/> <img src=\"ml_results/LR_performace_exc_feats.png\" width=\"60%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that Linear Regression performed more favourably on the training and testing data including latitude and longitude. This is somewhat to be expected, as areas with expensive median house values often border other areas with similar socioeconomic groups.\n",
    "\n",
    "To attempt to address the multicollinearity, we also ran Recursive Feature Elimination excluding longitude, latitude, and total bedrooms, which was the feature that had the highest Variance Inflation Factor.\n",
    "\n",
    "![Image](ml_results/LR_performace_exc_feats_2.png)\n",
    "\n",
    "As expected, the results are very similar to the model that only excluded latitude and longitude because the information from the feature \"total_bedrooms\" is effectively redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also attempted to fit a K-Nearest Neighbor to our data, and KNN yielded better accuracy than simple linear regression. A Standard Scaler was used to pre-process the data, which likely contributed to the success of KNN. The following demonstrates the relationship between the number of nearest neighbours and the resulting training and testing scores. As with Linear Regression, we removed Latitude and Longitude in hopes to see the effect it had on KNN in terms of spatial nearest neightbours, and the results are as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_results/KNN_performace.png\" width=\"40%\" align=\"left\"/> <img src=\"ml_results/KNN_performace_exc_feats.png\" width=\"55%\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we can infer that having latitude and longitude included did improve the KNN model. With or without these features, the number of nearest neighbours that should be used is approximately 9 in order to avoid overfitting.\n",
    "\n",
    "The goal of our project is not to predict based on Census data for other states, however the results are still quite effective without latitude and longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Areas for Improvement\n",
    "Given the time constraints for this project, we chose to limit the number of models created and we only scratched the surface of apparent multicollinearity issues that exist in this dataset. In the future, the following may be areas for expansion or improvement:\n",
    "- Expand the models to try - we would have expanded with a Support Vector Machine.\n",
    "- Use cross-validation scores for each model to get a better understanding of model performance.\n",
    "- We took out latitude, longitude, and total bedrooms for different parts of the analysis, but there may have been more optimal combinations of fewer features. In an expansion of this project, regularization should be explored, and different combinations of features should be tried to understand the roles of each of the existing features.\n",
    "- The only measures of multicollinearity that we examined were the Variance Inflation Factor, and the correlation (which doesn't actually measure multicollinearity but shows how variables move together). In an expansion of this project, aside from just recursive feature elimination, we would recommend regularization with combinations that include and exclude latitude and longitude.\n",
    "- Feature interpretation techniques such as **Shap** and **eli5** can be used to explain the predictions.\n",
    "\n",
    "### Conclusion\n",
    "In both our linear regression model and K-Nearest Neighbors model, we achieved higher accuracy than Eric Chen's best score of 0.60, which was with linear regression. Chen did not fit a KNN model, so it is unclear whether this model would have performed better for him. \n",
    "\n",
    "The following illustrates the distributions of the actual and predicted median house prices, as generated by the KNN model that included all features.\n",
    "\n",
    "![Image](ml_results/KNN_actual_vs_predicted.png)\n",
    "\n",
    "It appears that the inaccurate predictions largely came from the $500,000 cap on median house values. For the purposes of predicting median housing price in California by census block, the linear regression and KNN models are effective at estimating the response. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Balla, Deepanshu. n.d. SPLITTING Data into Training and Test Sets with R. https://www.listendata.com/2015/02/splitting-data-into-training-and-test.html.\n",
    "\n",
    "de Jonge, Edwin 2018. docopt: Command-Line Interface Specification Language. https://CRAN.R-project.org/package=docopt.\n",
    "\n",
    "Kuhn, Max. 2020. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret.\n",
    "\n",
    "Lang, Michael 2017. checkmate: Fast Argument Checks for Defensive R Programming. https://journal.r-project.org/archive/2017/RJ-2017-028/index.html.\n",
    "\n",
    "R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n",
    "\n",
    "Wickham, Hadley. 2011. testthat: Get Started with Testing. https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf.\n",
    "\n",
    "Wickham, Hadley. 2017. Tidyverse: Easily Install and Load the ‚ÄôTidyverse‚Äô. https://CRAN.R-project.org/package=tidyverse.\n",
    "\n",
    "Wickham, Hadley, and Lionel Henry. 2019. Tidyr: Tidy Messy Data. https://CRAN.R-project.org/package=tidyr.\n",
    "\n",
    "Wickham, Hadley, Jim Hester, and Romain Francois. 2018. Readr: Read Rectangular Text Data. https://CRAN.R-project.org/package=readr.\n",
    "\n",
    "Fabian Pedregosa, Ga√´l Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, √âdouard Duchesnay; 12(Oct):2825‚àí2830, 2011 https://scikit-learn.org/stable/\n",
    "\n",
    "Bernard J. (2016) Python Data Analysis with pandas. In: Python Recipes Handbook. Apress, Berkeley, CA. https://pandas.pydata.org/\n",
    "\n",
    "Pedregosa et al., 2011. Scikit-learn: Machine Learning in Python, JMLR 12, pp. 2825-2830. http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n",
    "\n",
    "Seabold, Skipper, and Josef Perktold, 2010. ‚Äústatsmodels: Econometric and statistical modeling with python.‚Äù Proceedings of the 9th Python in Science Conference. http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf\n",
    "\n",
    "VanderPlas, Jacob & Granger, Brian & Heer, Jeffrey & Moritz, Dominik & Wongsuphasawat, Kanit & Lees, Eitan & Timofeev, Ilia & Welsh, Ben & Sievert, Scott. (2018). Altair: Interactive Statistical Visualizations for Python. Journal of Open Source Software. 3. 1057. 10.21105/joss.01057. https://altair-viz.github.io/_sources/index.rst.txt\n",
    "\n",
    "plightbo, simon.m.stewart, hbchai, jrhuggins, et al. ¬© Copyright 2011. https://selenium.dev/documentation/en/front_matter/copyright_and_attributions/\n",
    "\n",
    "Oliphant, T. E. (2006). A guide to NumPy (Vol. 1). Trelgol Publishing USA. https://web.mit.edu/dvp/Public/numpybook.pdf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
