
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Predicting-Median-House-Prices-in-California-Census-Blocks-(Census-1990)">Predicting Median House Prices in California Census Blocks (Census 1990)<a class="anchor-link" href="#Predicting-Median-House-Prices-in-California-Census-Blocks-(Census-1990)">&#182;</a></h1><p>Author: DSCI 522 Group 312</p>
<p>Date: January 25, 2020</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Summary">Summary<a class="anchor-link" href="#Summary">&#182;</a></h2><p>This analysis focuses on predicting the median house prices in census blocks given independent variable about the location, home characteristics, and the demographics of the census block. This dataset was sourced from Kaggle, and many other people have completed <a href="https://www.kaggle.com/camnugent/california-housing-prices/kernels">similar analyses</a>.</p>
<p>Our goal is to build a model that will predict median house value with a higher model validation score than the 0.60 achieved by <a href="https://www.kaggle.com/ericfeng84">Eric Chen</a>, the author of <a href="https://www.kaggle.com/ericfeng84/the-california-housing-price">The California House Price</a> Kaggle page from which the dataset was obtained.</p>
<p>We aim to bring additional insight to the existing models including looking at multicollinearity and trying KNN with a variety of different values for n_neighbors.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Methods">Methods<a class="anchor-link" href="#Methods">&#182;</a></h2><h3 id="Data">Data<a class="anchor-link" href="#Data">&#182;</a></h3><p>This dataset is a modified version of <a href="https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html">The California Housing Dataset</a>, with <a href="https://github.com/ageron/handson-ml">additional columns added by Aurélien Geron</a>. This dataset contains information about median California house values per census block as sourced from the 1990 US Census.</p>
<h3 id="Analysis">Analysis<a class="anchor-link" href="#Analysis">&#182;</a></h3><p>We used Linear Regression, K-Nearest Neighbour Trees, and a Random Forest Regressor to predict the median house value given the independent variables.</p>
<h3 id="Results-and-Discussion">Results and Discussion<a class="anchor-link" href="#Results-and-Discussion">&#182;</a></h3><p>The Exploratory Data Analysis focused on identifying linear relationships between the independent variables and the dependent variable as well as looking at correlations between independent variables. Previous analyses of this dataset highlighted that linear regression was an appropriate prediction method for the median housing value, but generally, they lacked insight into multicollinearity (the correlation and linear relationships between independent variables). Of all of the variables examined, the Variance Inflation Factor (VIF) was higher than 1, which means that there is strong evidence of multicollinearity.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;eda_charts/vif_table.csv&#39;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

    <div class="prompt output_prompt">Out[3]:</div>



<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variable</th>
      <th>vif_val</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>housing_median_age</td>
      <td>1.163905</td>
    </tr>
    <tr>
      <th>1</th>
      <td>total_rooms</td>
      <td>11.849443</td>
    </tr>
    <tr>
      <th>2</th>
      <td>total_bedrooms</td>
      <td>34.891047</td>
    </tr>
    <tr>
      <th>3</th>
      <td>population</td>
      <td>6.582837</td>
    </tr>
    <tr>
      <th>4</th>
      <td>households</td>
      <td>33.871693</td>
    </tr>
    <tr>
      <th>5</th>
      <td>median_income</td>
      <td>1.524263</td>
    </tr>
    <tr>
      <th>6</th>
      <td>intercept</td>
      <td>18.278944</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>The variable with the highest VIF is total bedrooms, and this appears to be strongly linearly related to the total number of rooms, given that the room count includes the bedrooms.</p>
<p><img src="eda_charts/total-rooms_total-bedrooms.png" alt="Image"></p>
<p>The following heatmap represents the correlation values of the variables.</p>
<p><img src="eda_charts/correlation_heatmap.png" alt="Image"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A common approach to address multicollinearity is to remove variables with high VIFs. As is common in this case, the Linear Regression model performed best when all (or all but one) of the variables were included. The following illustrates the Recursive Feature Elimination for a Linear Regression Model. The x-axis represents the number of features selected so far.</p>
<p>We additionally ran Recursive Feature Elimination on a Linear Regression model, excluding Latitude and Longitude, since these features are very specific to California. The results follow.</p>
<p>It is clear that Linear Regression performed more favourably on the training and testing data including latitude and longitude. This is somewhat to be expected, as areas with expensive median house values often border other areas with similar socioeconomic groups.</p>
<p>To attempt to address the multicollinearity, we also ran Recursive Feature Elimination excluding longitude, latitude, and total bedrooms, which was the feature that had the highest Variance Inflation Factor.</p>
<p><img src="ml_results/LR_performace_exc_feats_2.png" alt="Image"></p>
<p>As expected, the results are very similar to the model that only excluded latitude and longitude because the information from the feature "total_bedrooms" is effectively redundant.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="ml_results/LR_performace.png" width="3%" align="left"/> <img src="ml_results/LR_performace_exc_feats.png" width="50%" align="left"/></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We also attempted to fit a K-Nearest Neighbor to our data, and KNN yielded better accuracy than simple linear regression. A Standard Scaler was used to pre-process the data, which likely contributed to the success of KNN. The following demonstrates the relationship between the number of nearest neighbours and the resulting training and testing scores.</p>
<p><img src="ml_results/KNN_performace.png" alt="Image"></p>
<p>As with Linear Regression, we removed Latitude and Longitude in hopes to see the effect it had on KNN in terms of spatial nearest neightbours, and the results are as follows.</p>
<p><img src="ml_results/KNN_performace_exc_feats.png" alt="Image"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>From the above, we can infer that having latitude and longitude included did improve the KNN model. With or without these features, the number of nearest neighbours that should be used is approximately 9 in order to avoid overfitting.</p>
<p>The goal of our project is not to predict based on Census data for other states, however the results are still quite effective without latitude and longitude.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Areas-for-Improvement">Areas for Improvement<a class="anchor-link" href="#Areas-for-Improvement">&#182;</a></h3><p>Opportunities for improvement of the predictive model include:</p>
<ul>
<li>Increasing the breadth of Machine Learning Models used to predict the median housing value;</li>
<li>Obtaining cross-validation scores, rather than the simple score;</li>
<li>Work through a feature engineering process to make features more relevant;</li>
<li>Use a feature selection method for linear regression that addresses or discourages multicollinearity;</li>
<li>Conducting more in-depth analysis to address multicollinearity.</li>
</ul>
<h3 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion">&#182;</a></h3><p>In both our linear regression model and K-Nearest Neighbors model, we achieved higher accuracy than Eric Chen's best score, which was with linear regression. Chen did not fit a KNN model, so it is unclear whether this model would have performed better for him. For the purposes of predicting median housing price in California by census block, the linear regression and KNN models are effective at estimating the response.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="References">References<a class="anchor-link" href="#References">&#182;</a></h2><p>Balla, Deepanshu. n.d. SPLITTING Data into Training and Test Sets with R. <a href="https://www.listendata.com/2015/02/splitting-data-into-training-and-test.html">https://www.listendata.com/2015/02/splitting-data-into-training-and-test.html</a>.</p>
<p>de Jonge, Edwin 2018. docopt: Command-Line Interface Specification Language. <a href="https://CRAN.R-project.org/package=docopt">https://CRAN.R-project.org/package=docopt</a>.</p>
<p>Kuhn, Max. 2020. Caret: Classification and Regression Training. <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.</p>
<p>Lang, Michael 2017. checkmate: Fast Argument Checks for Defensive R Programming. <a href="https://journal.r-project.org/archive/2017/RJ-2017-028/index.html">https://journal.r-project.org/archive/2017/RJ-2017-028/index.html</a>.</p>
<p>R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
<p>Wickham, Hadley. 2011. testthat: Get Started with Testing. <a href="https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf">https://journal.r-project.org/archive/2011-1/RJournal_2011-1_Wickham.pdf</a>.</p>
<p>Wickham, Hadley. 2017. Tidyverse: Easily Install and Load the ’Tidyverse’. <a href="https://CRAN.R-project.org/package=tidyverse">https://CRAN.R-project.org/package=tidyverse</a>.</p>
<p>Wickham, Hadley, and Lionel Henry. 2019. Tidyr: Tidy Messy Data. <a href="https://CRAN.R-project.org/package=tidyr">https://CRAN.R-project.org/package=tidyr</a>.</p>
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2018. Readr: Read Rectangular Text Data. <a href="https://CRAN.R-project.org/package=readr">https://CRAN.R-project.org/package=readr</a>.</p>
<p>Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, Édouard Duchesnay; 12(Oct):2825−2830, 2011 <a href="https://scikit-learn.org/stable/">https://scikit-learn.org/stable/</a></p>
<p>Bernard J. (2016) Python Data Analysis with pandas. In: Python Recipes Handbook. Apress, Berkeley, CA. <a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a></p>
<p>Pedregosa et al., 2011. Scikit-learn: Machine Learning in Python, JMLR 12, pp. 2825-2830. <a href="http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html">http://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</a></p>
<p>Seabold, Skipper, and Josef Perktold, 2010. “statsmodels: Econometric and statistical modeling with python.” Proceedings of the 9th Python in Science Conference. <a href="http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf">http://conference.scipy.org/proceedings/scipy2010/pdfs/seabold.pdf</a></p>
<p>VanderPlas, Jacob &amp; Granger, Brian &amp; Heer, Jeffrey &amp; Moritz, Dominik &amp; Wongsuphasawat, Kanit &amp; Lees, Eitan &amp; Timofeev, Ilia &amp; Welsh, Ben &amp; Sievert, Scott. (2018). Altair: Interactive Statistical Visualizations for Python. Journal of Open Source Software. 3. 1057. 10.21105/joss.01057. <a href="https://altair-viz.github.io/_sources/index.rst.txt">https://altair-viz.github.io/_sources/index.rst.txt</a></p>
<p>plightbo, simon.m.stewart, hbchai, jrhuggins, et al. © Copyright 2011. <a href="https://selenium.dev/documentation/en/front_matter/copyright_and_attributions/">https://selenium.dev/documentation/en/front_matter/copyright_and_attributions/</a></p>
<p>Oliphant, T. E. (2006). A guide to NumPy (Vol. 1). Trelgol Publishing USA. <a href="https://web.mit.edu/dvp/Public/numpybook.pdf">https://web.mit.edu/dvp/Public/numpybook.pdf</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[&nbsp;]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span> 
</pre></div>

    </div>
</div>
</div>

</div>
 

